---
name: ai_client.py tool calling with LangGraph agent + async bridge
status: closed
created: 2026-02-26T16:12:18Z
updated: 2026-02-26T16:22:04Z
github: https://github.com/WOOWTECH/odoo-addon-woow-paas-platform/issues/101
depends_on: [97, 99]
parallel: false
conflicts_with: []
---

# Task: ai_client.py tool calling with LangGraph agent + async bridge

## Description

修改 `ai_client.py`，新增 tool calling 支援。使用 `langchain-mcp-adapters` 的 `MultiServerMCPClient` 取得 LangChain tools，透過 LangGraph `StateGraph` + `ToolNode` 建立輕量 agent。核心挑戰是 Odoo sync 環境中的 async bridge。

這是本 epic 的核心 task，也是技術風險最高的部分。

## Acceptance Criteria

- [ ] `AIClient` 新增 `chat_completion_with_tools(messages, mcp_tools)` 方法（同步，帶 tool calling）
- [ ] `AIClient` 新增 `chat_completion_stream_with_tools(messages, mcp_tools)` generator 方法
- [ ] 使用 LangGraph `StateGraph` + `ToolNode` + `tools_condition` 建立 agent
- [ ] `asyncio.run()` 正確包裝 MCP async 操作
- [ ] Tool calling loop 有 `recursion_limit=5` 防止無限迴圈
- [ ] Stream 版本 yield 三種 event：`text_chunk`, `tool_call`, `tool_result`
- [ ] 無 MCP tools 時退回原本的 `chat_completion` / `chat_completion_stream`（向後相容）
- [ ] Tool 執行失敗時，AI 收到 error message 並繼續對話（不 crash）

## Technical Details

### 修改檔案
- `src/models/ai_client.py` — 核心修改

### 新增方法設計

```python
class AIClient:
    def __init__(self, ...):
        self.llm = ChatOpenAI(...)

    def _build_mcp_client_config(self, mcp_tools):
        """從 mcp_tool records 建立 MultiServerMCPClient config dict."""
        servers = {}
        for tool in mcp_tools:
            server = tool.server_id
            if server.name not in servers:
                servers[server.name] = server._get_mcp_client_config()
        return servers

    def chat_completion_with_tools(self, messages, mcp_tools=None):
        """Sync tool calling with LangGraph agent."""
        if not mcp_tools:
            return self.chat_completion(messages)
        return asyncio.run(self._async_agent_invoke(messages, mcp_tools))

    def chat_completion_stream_with_tools(self, messages, mcp_tools=None):
        """Streaming tool calling. Yields dicts with type: text_chunk/tool_call/tool_result."""
        if not mcp_tools:
            for chunk in self.chat_completion_stream(messages):
                yield {"type": "text_chunk", "content": chunk}
            return
        yield from self._sync_stream_with_tools(messages, mcp_tools)

    async def _async_agent_invoke(self, messages, mcp_tools):
        """Build LangGraph agent and invoke."""
        server_config = self._build_mcp_client_config(mcp_tools)
        async with MultiServerMCPClient(server_config) as client:
            tools = await client.get_tools()
            # Filter to only enabled tools
            enabled_names = {t.name for t in mcp_tools}
            tools = [t for t in tools if t.name in enabled_names]

            graph = self._build_agent_graph(tools)
            result = await graph.ainvoke({"messages": messages})
            return result["messages"][-1].content

    def _build_agent_graph(self, tools):
        """Build LangGraph StateGraph with ToolNode."""
        from langgraph.graph import StateGraph, MessagesState, START
        from langgraph.prebuilt import ToolNode, tools_condition

        llm_with_tools = self.llm.bind_tools(tools)

        def call_model(state):
            return {"messages": [llm_with_tools.invoke(state["messages"])]}

        builder = StateGraph(MessagesState)
        builder.add_node("call_model", call_model)
        builder.add_node("tools", ToolNode(tools))
        builder.add_edge(START, "call_model")
        builder.add_conditional_edges("call_model", tools_condition)
        builder.add_edge("tools", "call_model")
        return builder.compile(recursion_limit=5)
```

### Streaming with Tools 策略

Streaming 版本比較複雜，因為 LangGraph 的 `.astream_events()` 或 `.astream()` 需要 async generator。方案：

1. 使用 `graph.astream(input, stream_mode="updates")` 取得每個 node 的 output
2. 當 `call_model` node 輸出含 `tool_calls` → yield `tool_call` events
3. 當 `tools` node 輸出 → yield `tool_result` events
4. 最後一次 `call_model` node 輸出 text → yield `text_chunk` events
5. 用 `asyncio.run()` 包裝，內部用 async generator 收集所有 events 後逐個 yield

### Async Bridge 注意事項

- Odoo 使用 `gevent` 或 `threading` 作為 WSGI server
- `asyncio.run()` 需要在沒有 running event loop 的 thread 中執行
- 如果遇到 "cannot run nested event loop" 錯誤，需改用 `nest_asyncio` 或 thread pool

## Dependencies

- [ ] Task #97 完成（`mcp_server._get_mcp_client_config()` 存在）
- [ ] Task #99 完成（`ai.assistant.get_enabled_mcp_tools()` 存在）
- [ ] `langchain-mcp-adapters` 已安裝
- [ ] `langgraph` 已安裝

## Effort Estimate

- Size: XL
- Hours: 12-16
- Parallel: false（核心 task）

## Definition of Done

- [ ] `chat_completion_with_tools()` 可成功調用 MCP tool 並回傳結果
- [ ] `chat_completion_stream_with_tools()` 正確 yield tool call events
- [ ] 無 tools 時向後相容
- [ ] Recursion limit 正確觸發
- [ ] Tool 執行失敗時 graceful handling
- [ ] 在 Odoo Docker 環境中 async bridge 正常運作
